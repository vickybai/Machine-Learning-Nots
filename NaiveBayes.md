贝叶斯定理要解决的其实是一个求逆向概率的问题。在现实生活中，我们在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。因为他们需要少量的训练数据来估计必要的参数。

## 贝叶斯定理

$$P(B|A) = \dfrac{P(A|B)P(B)}{P(A)}$$

**先验概率（prior probability）**：在事件发生前，求该事件发生的概率。例如：一个硬币的质量分布是均匀的。
**似然（Liklihood）**：似然就是从观察值来推测参数，换句话说，在数据分布函数未知的情况下，根据已有的观测值去推断该观测值来源于什么样的分布函数的概率。**似然函数是一个关于统计模型参数的函数**。已知一个观测到的结果去求一种参数设置对产生这个结果的概率是多少。对于结果 $$x$$ ，在参数集合 $$θ$$ 上的似然，就是在给定这些参数值的基础上，观察到的结果的概率 $$L(θ|x)=P(x|θ)$$。
**后验概率（posterior probability）**：是在得到观察值之后在重新加以修正的概率，也就是所谓的条件概率。$$Posterior \propto Likelihood *Prior$$（后验概率正比于似然乘以先验概率）。

然后我们再根据贝叶斯公式理解一下这些概念：$$P(H|D)=\dfrac{P(D|H)P(H)}{P(D)}$$。

$$P(D|H)$$ 就是似然函数，$$P(H)$$ 就是先验概率，$$P(D)$$ 起到了归一化（标准化）的作用，使数据之间具有可比较性而进行的标准化处理使其处于同一数量级。

| 表示       | 中文         | 意思 |
|:---------- |:-------------|:-----|
| $$P(H)$$   | 先验概率 | 在得到新数据之前某一假设的概率 |
| $$P(H/D)$$ | 后验概率 | 在看到新数据后，我们要计算的该假设的概率 |
| $$P(D/H)$$ | 似然     | 该假设下得到这一数据的概率|
| $$P(D)$$   | 标准化常量| 在任何假设下得到这一数据的概率|



# 朴素贝叶斯
在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出 $$Y$$ 和特征 $$X$$ 之间的关系，要么是决策函数 $$Y=f(X)$$ ,要么是条件分布 $$P(Y|X)$$。**但是朴素贝叶斯却是生成方法，也就是直接找出特征输出 $$Y$$ 和特征 $$X$$ 的联合分布 $$P(X,Y)$$ ,然后用 $$P(Y|X)=P(X,Y)/P(X)$$ 得出。** 也就是对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。

### 算法
1. 设 $$x = \{a_1, a_2, a_3, \cdots, a_n\}$$ 为一个待分类项，$$a_i$$ 是 $$x$$ 的一个特征属相。
2. 有类别集合 $$C = \{y_1, \cdots, y_n\}$$ 
3. 计算 $$P(y_1|x)$$, $$P(y_2|x)$$, ..., $$P(y_n|x)$$。其中 $$P(y_j|x_1,...,x_n) \propto P(y_j)\prod_{i=1}^n P(x_i|y_j)$$
4. 求 $$P(y_k|x) = \max\{P(y_1|x)$$, $$P(y_2|x)$$, ..., $$P(y_n|x)\}$$, 则 $$x \in y_k$$， 也就是 $$\hat y =argmax_y P(y)\prod_{i=1}^n P(x_i|y)$$

其中第3步中，求解 $$P(y_k|x)$$ 就用到了贝叶斯定理:     在训练数据集中，统计得到在各类别下各个特征属性的条件概率估计。即 $$P(a_1|y_1), P(a_2|y_1),.. ,P(a_n|y_1)$$, $$P(a_1|y_2), P(a_1|y_2), ... , P(a_n|y_2), ..., P(a_1|y_m), P(a_1|y_m), P(a_n|y_m)$$

贝叶斯定理说明了如下关系：

$$P(y|x_1,...,x_n) = \dfrac{P(y)P(x_1,...x_n|y)}{P(x_1,...x_n)} = \dfrac{P(y)\prod_{i=1}^n P(x_i|y)}{P(x_1,...x_n)}$$

由于输入的 $$P(x_1,...,x_n)$$ 是固定不变的，我们可以使用下面的分类规则：

$$P(y_j|x_1,...,x_n) \propto P(y_j)\prod_{i=1}^n P(x_i|y_j)$$

$$\hat y =argmax_y P(y)\prod_{i=1}^n P(x_i|y)$$

我们可以用最大后验估计（Maximum A Posteriori ,MAP）来估计 $$P(y)$$ 和 $$P(x_i|y)$$ ,其中 $$P(y)$$ 是训练集中 $$y$$ 的相对频率（也就是 $$y$$ 训练集总数）。不同的贝叶斯分类器对 $$P(x_i|y)$$ 的分布函数有不同的假设。

# sklearn 中的贝叶斯分类
在scikit-learn中，一共有3个朴素贝叶斯的分类算法类。分别是GaussianNB，MultinomialNB和BernoulliNB。这三种分类算法是按照先验的不同而划分产生的，其中GaussianNB就是先验为高斯分布的朴素贝叶斯
，MultinomialNB就是先验为多项式分布的朴素贝叶斯，而BernoulliNB就是先验为伯努利分布的朴素贝叶斯，伯努利分布要求每个特征只能取值0或者1。如果数据集不满足要求时, a BernoulliNB instance may binarize its input (depending on the binarize parameter).

### 代码
下面是sklearn的代码：
```python
def classify(features_train, labels_train):
    import numpy as np 
    from sklearn.naive_bayes 
    import GaussianNB 
    X = features_train 
    Y = labels_train 
    clf = GaussianNB() 
    clf.fit(X, Y) 
    return clf 
```

## 贝叶斯理论的不足

1. 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下,假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。

2. 有些情况下，我们可以基于现有背景进行得知先验概率。但在大部分情况下，先验概率是偏主观性的。这也是频率学派提出的对贝叶斯学派的批评之一。另外，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。

3. 由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。

4. 对输入数据的表达形式很敏感。
 




##附录 （常见的先验函数）

1. 贝塔分布
2. 


use mcmc with mh lagorithm to generate sample from the posterior distribution
monte carlo trace plot

markov chain

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


