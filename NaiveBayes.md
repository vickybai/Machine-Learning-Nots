


贝叶斯定理要解决的其实是一个求逆向概率的问题。在现实生活中，我们
在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。



## 贝叶斯定理

$$P(B|A) = \dfrac{P(A|B)P(B)}{P(A)}$$

**先验概率（prior probability）**：在事件发生前，求该事件发生的概率。
**似然（Liklihood）**：似然就是从观察值来推测参数，换句话说，在数据分布函数未知的情况下，根据已有的观测值去推断该观测值来源于什么样的分布函数的概率。**似然函数是一个关于统计模型参数的函数**。已知一个观测到的结果去求一种参数设置对产生这个结果的概率是多少。对于结果 $$x$$ ，在参数集合 $$θ$$ 上的似然，就是在给定这些参数值的基础上，观察到的结果的概率 $$L(θ|x)=P(x|θ)$$。
**后验概率（posterior probability）**：是在得到观察值之后在重新加以修正的概率，也就是所谓的条件概率。$$Posterior \propto Likelihood *Prior$$（后验概率正比于似然乘以先验概率）。
用根据贝叶斯公式理解一下：$$P(H|E)=\dfrac{P(E|H)P(H)}{P(E)}$$。
$$P(E|H)$$ 就是似然函数，$$P(H)$$ 就是先验概率，$$P(E)$$ 起到了归一化的作用。啥叫归一化，简单来说就是为了使数据之间具有可比较性而进行的标准化处理使其处于同一数量级，所以归一化也叫标准化。


# sklearn 中的贝叶斯分类
在scikit-learn中，一共有3个朴素贝叶斯的分类算法类。分别是GaussianNB，MultinomialNB和BernoulliNB。这三种分类算法是按照先验的不同而划分产生的，其中GaussianNB就是先验为高斯分布的朴素贝叶斯，MultinomialNB就是先验为多项式分布的朴素贝叶斯，而BernoulliNB就是先验为伯努利分布的朴素贝叶斯。

# 朴素贝叶斯分类 
朴素贝叶斯方法是一种基于贝叶斯定理和每对特征间独立假设的监督学习算法。给定一个类变量和一个从属特征向量，贝叶斯定理说明了如下关系：

P(y|x1,...,xn)=P(y)P(x1,...xn|y)P(x1,...xn)













$$xsasd$$







<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>











