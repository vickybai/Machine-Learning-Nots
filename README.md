# Machine-Learning-Nots

机器学习理论的根本假设是，所有的样本都是从一个固定但未知的概率分布（记为 D）以独立同分布（i.i.d.）的方式生成的。同分布的假设确保了观测数据是平稳的，独立性的假设则明确了每一个单独的样本都携带了解决预测问题的最大信息。根据这个假设，训练集 S 中的所有样本 (xi, yi) 都是服从 D 并且独立同分布的。换句话说，每一个训练集都是一个由服从 D 的独立同分布样本组成的样本集。
